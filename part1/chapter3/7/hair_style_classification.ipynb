{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hair style classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드를 고정합니다.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = 'mps'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json\n",
    "\n",
    "root_path = '/Users/kimhongseok/cv_79_projects/part1/chapter3/7/data'\n",
    "annot_path = os.path.join(root_path, 'annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_path_list = list(glob(f'{annot_path}/**/*.json', recursive=True))\n",
    "len(annotation_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(annotation_path_list[0], 'r') as json_f:\n",
    "    sample_js = json.load(json_f)\n",
    "\n",
    "sample_js"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## annotation dataframe 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_path_list[0].split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'/'.join(annotation_path_list[0].split('/')[9:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "select_categories = ['basestyle', 'length', 'color', 'sex']\n",
    "new_annotations = pd.DataFrame(columns=['filename']+select_categories)\n",
    "filename_list = []\n",
    "catefory_list_map = defaultdict(list)\n",
    "\n",
    "for idx, annot_path in enumerate(annotation_path_list):\n",
    "    sub_dir = '/'.join(annot_path.split('/')[9:-1])\n",
    "    with open(annot_path, 'r') as f:\n",
    "        js = json.load(f)\n",
    "\n",
    "    image_filename = js['filename']\n",
    "    image_path = os.path.join(sub_dir, image_filename)\n",
    "    filename_list.append(image_path)\n",
    "    for cat in select_categories:\n",
    "        catefory_list_map[cat].append(js[cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_annotations['filename'] = filename_list\n",
    "for cat, cat_list in catefory_list_map.items():\n",
    "    new_annotations[cat] = cat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class를 one-hot vector로 표현\n",
    "\n",
    "columns = new_annotations.columns.tolist()\n",
    "new_annotations = pd.get_dummies(new_annotations, columns=columns[1:], dtype=int)\n",
    "new_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_annotations.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_annotations.to_csv('/Users/kimhongseok/cv_79_projects/part1/chapter3/7/data/annotations/annotations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('/Users/kimhongseok/cv_79_projects/part1/chapter3/7/data/annotations/annotations.csv')\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.sample(frac=1).reset_index(drop=True)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_path, data_df, classes, transforms):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        self.transforms = transforms\n",
    "\n",
    "        l = data_df.shape[0]\n",
    "        for i in range(l):\n",
    "            img_path = os.path.join(root_path, data_df.iloc[i, 0])\n",
    "            classes = torch.tensor(data_df.iloc[i, 1:].tolist()).float()\n",
    "            self.data.append([img_path, classes])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.data[idx][0]).convert('RGB')\n",
    "        img = self.transforms(img)\n",
    "        classes = self.data[idx][1]\n",
    "\n",
    "        return img, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.iloc[:1000]\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor()\n",
    "])\n",
    "\n",
    "classes = data_df.columns[1:]\n",
    "root_path = '/Users/kimhongseok/cv_79_projects/part1/chapter3/7/data/images'\n",
    "\n",
    "total_dataset = CustomDataset(root_path, data_df, classes, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "plt.imshow(total_dataset[0][0].permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num = len(total_dataset)\n",
    "train_num, valid_num, test_num = int(total_num*0.8), int(total_num*0.1), int(total_num*0.1)\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(total_dataset, [train_num, valid_num, test_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=100, shuffle=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training, evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_dataloader, criterion, optimizer, threshold, epoch, num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    total_labels = []\n",
    "    total_preds = []\n",
    "\n",
    "    tbar = tqdm(train_dataloader)\n",
    "    for images, labels in tbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        preds = (torch.sigmoid(outputs) > threshold).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        total_labels.extend(labels.cpu().numpy())\n",
    "        total_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "        tbar.set_description(f'Epoch/Epochs [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}')\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_f1 = f1_score(total_labels, total_preds, average='macro')\n",
    "\n",
    "    return model, train_loss, train_f1\n",
    "\n",
    "def evaluation(model, valid_dataloader, criterion, threshold, epoch, num_epochs):\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    total_labels = []\n",
    "    total_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tbar = tqdm(valid_dataloader)\n",
    "        for images, labels in tbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            preds = (torch.sigmoid(outputs) > threshold).float()\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "            total_labels.extend(labels.cpu().numpy())\n",
    "            total_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            tbar.set_description(f'Epoch/Epochs [{epoch+1}/{num_epochs}], Valid Loss: {loss.item():.4f}')\n",
    "\n",
    "    valid_loss /= len(valid_dataloader)\n",
    "    valid_f1 = f1_score(total_labels, total_preds, average='macro')\n",
    "\n",
    "    return model, valid_loss, valid_f1\n",
    "\n",
    "def training_loop(model, train_dataloader, valid_dataloader, criterion, optimizer, threshold, num_epochs):\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model, train_loss, train_f1 = training(model, train_dataloader, criterion, optimizer, threshold, epoch, num_epochs)\n",
    "        model, valid_loss, valid_f1 = evaluation(model, valid_dataloader, criterion, threshold, epoch, num_epochs)\n",
    "\n",
    "        print(f'Train Loss: {train_loss}, Train F1: {train_f1}, Valid Loss: {valid_loss}, Valid F1: {valid_f1}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet34(pretrained=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model.fc = torch.nn.Linear(in_features=4096, out_features=len(classes), bias=True)\n",
    "model.fc.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "threshold = 0.5\n",
    "num_epochs = 10\n",
    "\n",
    "model = training_loop(model, train_dataloader, valid_dataloader, criterion, optimizer, threshold, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
