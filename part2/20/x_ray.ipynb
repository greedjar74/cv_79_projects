{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í•™ìŠµ, ê²€ì¦, í…ŒìŠ¤íŠ¸ ë°ì´í„° ê²½ë¡œ ìƒì„±\n",
    "- ìƒˆë¡œìš´ ê²½ë¡œ ìƒì„±\n",
    "- ì´ë¯¸ì§€ ë° ì–´ë…¸í…Œì´ì…˜ íŒŒì¼ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_data_path = '/Users/kimhongseok/cv_79_projects/part2/20/origin_data'\n",
    "pjt_path = '/Users/kimhongseok/cv_79_projects/part2/20'\n",
    "data_path = os.path.join(pjt_path, 'data')\n",
    "pjt_name = 'sec'\n",
    "\n",
    "train_root = os.path.join(data_path, 'train')\n",
    "valid_root = os.path.join(data_path, 'valid')\n",
    "test_root = os.path.join(data_path, 'test')\n",
    "\n",
    "cls_list = os.listdir(os.path.join(origin_data_path, 'images'))\n",
    "cls_list.remove('.DS_Store')\n",
    "\n",
    "for folder in [train_root, valid_root, test_root]:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        for s in ['images', 'labels']:\n",
    "            s_folder = os.path.join(folder, s)\n",
    "            if not os.path.exists(s_folder):\n",
    "                os.makedirs(s_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4175"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list = glob.glob(f'{origin_data_path}/images/*/*/*.png')\n",
    "len(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "835.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4175 / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding box í˜•íƒœ ë³€í™˜: xml -> yolo\n",
    "\n",
    "def xml_to_yolo_bbox(bbox, w, h):\n",
    "    x_center = (bbox[0] + bbox[2])/2/w # w: ì´ë¯¸ì§€ì˜ ê°€ë¡œí¬ê¸° -> wë¡œ ë‚˜ëˆ ì£¼ë©´ normalizationì´ ëœë‹¤.\n",
    "    y_center = (bbox[1] + bbox[3])/2/h\n",
    "    width = (bbox[2] - bbox[0])/w\n",
    "    height = (bbox[3] - bbox[1])/h\n",
    "\n",
    "    return [x_center, y_center, width, height]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xml -> txt\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "file_name = file_list[0].split('/')[-1].replace('png', 'xml')\n",
    "save_name = file_name.replace('xml', 'txt')\n",
    "file_path = f'{origin_data_path}/Annotation/Train/Pascal/Astrophysics/{file_name}'\n",
    "save_path = f'{origin_data_path}/Label/{save_name}'\n",
    "\n",
    "result = []\n",
    "\n",
    "tree = ET.parse(file_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "width = int(root.find('size').find('width').text)\n",
    "height = int(root.find('size').find('height').text)\n",
    "\n",
    "for obj in root.findall('object'):\n",
    "    label = obj.find('name').text\n",
    "    if label in cls_list:\n",
    "        index = cls_list.index(label)\n",
    "        bbox = [int(x.text) for x in obj.find('bndbox')]\n",
    "        yolo_bbox = xml_to_yolo_bbox(bbox, width, height)\n",
    "        bbox_string = ' '.join([str(x) for x in yolo_bbox])\n",
    "        result.append(f'{index} {bbox_string}')\n",
    "\n",
    "if result:\n",
    "    with open(save_path, 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‹œìž‘ì‹œìž‘\n",
      "\n",
      "ì‹œìž‘\n",
      "ì‹œìž‘\n",
      "ì‹œìž‘\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f4103ccdff4439b66e7a66e6d640d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/835 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506a2d98b2ac442989483091dc71c218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/835 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e448ab44c1042a2b0ac1b35215fe901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/835 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17ebccae9ad4a6291411db49edf68cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/835 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad95ac94bcca4ddf96d44351daefe85c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/835 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë\n",
      "ë\n",
      "ë\n",
      "ë\n",
      "ë\n"
     ]
    }
   ],
   "source": [
    "# ë©€í‹° ì“°ë ˆë“œë¥¼ ì‚¬ìš©í•´ì„œ ë¹ ë¥´ê²Œ ì²˜ë¦¬\n",
    "import concurrent.futures\n",
    "import warnings\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    print('ì‹œìž‘')\n",
    "    tbar = tqdm(chunk)\n",
    "    for file in tbar:\n",
    "        file_name = file.split('/')[-1].replace('png', 'xml')\n",
    "        save_name = file_name.replace('xml', 'txt')\n",
    "        file_path = f'{origin_data_path}/Annotation/Train/Pascal/Astrophysics/{file_name}'\n",
    "        save_path = f'{origin_data_path}/Label/{save_name}'\n",
    "\n",
    "        result = []\n",
    "\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        width = int(root.find('size').find('width').text)\n",
    "        height = int(root.find('size').find('height').text)\n",
    "\n",
    "        for obj in root.findall('object'):\n",
    "            label = obj.find('name').text\n",
    "            if label in cls_list:\n",
    "                index = cls_list.index(label)\n",
    "                bbox = [int(x.text) for x in obj.find('bndbox')]\n",
    "                yolo_bbox = xml_to_yolo_bbox(bbox, width, height)\n",
    "                bbox_string = ' '.join([str(x) for x in yolo_bbox])\n",
    "                result.append(f'{index} {bbox_string}')\n",
    "\n",
    "        if result:\n",
    "            with open(save_path, 'w', encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(result))\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as excutor:\n",
    "    futures = []\n",
    "    for start in range(0, 4175, 4175//5):\n",
    "        end = start+(4175//5)\n",
    "        chunk = file_list[start:end]\n",
    "        future = excutor.submit(process_chunk, chunk)\n",
    "        futures.append(future)\n",
    "        del future\n",
    "        gc.collect()\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        future.result()\n",
    "        print('ë')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import random\n",
    "\n",
    "random.seed(74)\n",
    "img_file_list = glob.glob('/Users/kimhongseok/cv_79_projects/part2/20/origin_data/images/*/*/*.png')\n",
    "file_list = []\n",
    "\n",
    "for file in img_file_list:\n",
    "    label_name = file.split('/')[-1].replace('png', 'txt')\n",
    "    label_path = f'{origin_data_path}/Label/{label_name}'\n",
    "    \n",
    "    if os.path.isfile(label_path):\n",
    "        file_list.append(file)\n",
    "\n",
    "random.shuffle(file_list)\n",
    "test_ratio = 0.1\n",
    "num_file = len(file_list)\n",
    "\n",
    "test_list = file_list[:int(num_file * test_ratio)]\n",
    "valid_list = file_list[:int(num_file * test_ratio):int(num_file*test_ratio)*2]\n",
    "train_list = file_list[:int(num_file * test_ratio)*2:]\n",
    "\n",
    "for i in test_list:\n",
    "    label_name = i.split('/')[-1].replace('png', 'txt')\n",
    "    label_path = f'{origin_data_path}/Label/{label_name}'\n",
    "    shutil.copyfile(label_path, f'{test_root}/labels/{label_name}') \n",
    "    img_name = i.split('/')[-1]\n",
    "    shutil.copyfile(i, f'{test_root}/images/{img_name}')\n",
    "\n",
    "for i in valid_list:\n",
    "    label_name = i.split('/')[-1].replace('png', 'txt')\n",
    "    label_path = f'{origin_data_path}/Label/{label_name}'\n",
    "    shutil.copyfile(label_path, f'{valid_root}/labels/{label_name}') \n",
    "    img_name = i.split('/')[-1]\n",
    "    shutil.copyfile(i, f'{valid_root}/images/{img_name}')\n",
    "\n",
    "for i in train_list:\n",
    "    label_name = i.split('/')[-1].replace('png', 'txt')\n",
    "    label_path = f'{origin_data_path}/Label/{label_name}'\n",
    "    shutil.copyfile(label_path, f'{train_root}/labels/{label_name}') \n",
    "    img_name = i.split('/')[-1]\n",
    "    shutil.copyfile(i, f'{train_root}/images/{img_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config íŒŒì¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pjt_root = '/Users/kimhongseok/cv_79_projects/part2/20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "data = dict()\n",
    "\n",
    "data['train'] = train_root\n",
    "data['val'] = valid_root\n",
    "data['test'] = test_root\n",
    "data['nc'] = len(cls_list)\n",
    "data['names'] = cls_list\n",
    "\n",
    "with open(f'{pjt_root}/security_screening.yaml', 'w') as f:\n",
    "    yaml.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.66 ðŸš€ Python-3.11.7 torch-2.3.0 CPU (Apple M1 Pro)\n",
      "Setup complete âœ… (10 CPUs, 16.0 GB RAM, 448.9/460.4 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pjt_root = '/Users/kimhongseok/cv_79_projects/part2/20'\n",
    "\n",
    "origin_data_path = '/Users/kimhongseok/cv_79_projects/part2/20/origin_data'\n",
    "data_root = '/Users/kimhongseok/cv_79_projects/part2/20/data'\n",
    "pjt_name = 'sec'\n",
    "\n",
    "train_root = os.path.join(data_root, 'train')\n",
    "valid_root = os.path.join(data_root, 'valid')\n",
    "test_root = os.path.join(data_root, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kimhongseok/cv_79_projects/part2/20\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/kimhongseok/cv_79_projects/part2/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.90 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.66 ðŸš€ Python-3.11.7 torch-2.3.0 CPU (Apple M1 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=security_screening.yaml, epochs=1, time=None, patience=100, batch=16, imgsz=384, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train3\n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2117983  ultralytics.nn.modules.head.Detect           [5, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11,137,535 parameters, 11,137,519 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "WARNING âš ï¸ Comet installed but not initialized correctly, not logging this run. Comet.ml requires an API key. Please provide as the first argument to Experiment(api_key) or as an environment variable named COMET_API_KEY \n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train3', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/kimhongseok/cv_79_projects/part2/20/data/train/labels.cache... 834 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 834/834 [00:00<?, ?it/s]\n",
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.14 (you have 1.4.12). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/kimhongseok/cv_79_projects/part2/20/data/valid/labels.cache... 1 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001111, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 384 train, 384 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1         0G      1.597      2.889      1.086         11        384: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [04:01<00:00,  4.55s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          1          2       0.98          1      0.995      0.747\n",
      "\n",
      "1 epochs completed in 0.068 hours.\n",
      "Optimizer stripped from runs/detect/train3/weights/last.pt, 22.5MB\n",
      "Optimizer stripped from runs/detect/train3/weights/best.pt, 22.5MB\n",
      "\n",
      "Validating runs/detect/train3/weights/best.pt...\n",
      "Ultralytics YOLOv8.2.66 ðŸš€ Python-3.11.7 torch-2.3.0 CPU (Apple M1 Pro)\n",
      "Model summary (fused): 168 layers, 11,127,519 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 13.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          1          2      0.981          1      0.995      0.747\n",
      "        Throwing Knife          1          2      0.981          1      0.995      0.747\n",
      "Speed: 0.1ms preprocess, 41.3ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('yolov8s.pt')\n",
    "results = model.train(data='security_screening.yaml', epochs=1, batch=16, imgsz=384)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.66 ðŸš€ Python-3.11.7 torch-2.3.0 CPU (Apple M1 Pro)\n",
      "Model summary (fused): 168 layers, 11,127,519 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/kimhongseok/cv_79_projects/part2/20/data/test/labels... 417 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 417/417 [00:00<00:00, 430.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /Users/kimhongseok/cv_79_projects/part2/20/data/test/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27/27 [00:58<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        417        717      0.374      0.471      0.377      0.265\n",
      "               Thinner         84        139      0.387      0.187      0.204      0.108\n",
      "        Throwing Knife         84        142      0.254      0.296      0.183     0.0911\n",
      "              ZippoOil         89        154      0.246      0.526      0.303      0.212\n",
      "               Battery         75        128      0.347      0.359      0.227      0.109\n",
      "                   HDD         93        154      0.635      0.987      0.969      0.804\n",
      "Speed: 0.1ms preprocess, 115.3ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val\u001b[0m\n",
      "map50-95 0.2647477374204408\n",
      "map50 0.37704859466739543\n"
     ]
    }
   ],
   "source": [
    "model = YOLO('/Users/kimhongseok/cv_79_projects/part2/20/runs/detect/train3/weights/best.pt')\n",
    "\n",
    "metrics = model.val(split='test')\n",
    "\n",
    "print('map50-95', metrics.box.map)\n",
    "print('map50', metrics.box.map50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils.plotting import Annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kimhongseok/cv_79_projects/part2/20\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/kimhongseok/cv_79_projects/part2/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_list = glob.glob('/Users/kimhongseok/cv_79_projects/part2/20/data/test/images/*')\n",
    "random.shuffle(test_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<generator object <listcomp>.<genexpr> at 0x351e698c0>, <generator object <listcomp>.<genexpr> at 0x37ab55ee0>, <generator object <listcomp>.<genexpr> at 0x37aef3680>, <generator object <listcomp>.<genexpr> at 0x37aef3a00>, <generator object <listcomp>.<genexpr> at 0x37aef3920>]\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "model = YOLO('/Users/kimhongseok/cv_79_projects/part2/20/runs/detect/train3/weights/best.pt')\n",
    "\n",
    "IMG_SIZE = (384, 384)\n",
    "test_data_transforms = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Resize(IMG_SIZE)\n",
    "])\n",
    "\n",
    "color_dict = [(random.randint(0, 255) for _ in range(3)) for _ in range(len(model.names))]\n",
    "print(color_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x384 1 ZippoOil, 45.6ms\n",
      "Speed: 1.2ms preprocess, 45.6ms inference, 0.4ms postprocess per image at shape (1, 3, 224, 384)\n"
     ]
    }
   ],
   "source": [
    "test_img = cv2.imread(test_file_list[0])\n",
    "img_src = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    "results = model(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x37af59310>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.imshow(img_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 224x384 1 ZippoOil, 38.2ms\n",
      "Speed: 1.1ms preprocess, 38.2ms inference, 0.3ms postprocess per image at shape (1, 3, 224, 384)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'rectangle'\n> Overload resolution failed:\n>  - Scalar value for argument 'color' is not numeric\n>  - Scalar value for argument 'color' is not numeric\n>  - argument for rectangle() given by name ('thickness') and position (4)\n>  - argument for rectangle() given by name ('thickness') and position (4)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m     b \u001b[38;5;241m=\u001b[39m box\u001b[38;5;241m.\u001b[39mxyxy[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m box\u001b[38;5;241m.\u001b[39mcls\n\u001b[0;32m---> 13\u001b[0m     annotator\u001b[38;5;241m.\u001b[39mbox_label(b, model\u001b[38;5;241m.\u001b[39mnames[\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mcls\u001b[39m)], color_dict[\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mcls\u001b[39m)])\n\u001b[1;32m     15\u001b[0m img_src \u001b[38;5;241m=\u001b[39m annotator\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m     16\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(img_src)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/ultralytics/utils/plotting.py:315\u001b[0m, in \u001b[0;36mAnnotator.box_label\u001b[0;34m(self, box, label, color, txt_color, rotated)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     p1, p2 \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m(box[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mint\u001b[39m(box[\u001b[38;5;241m1\u001b[39m])), (\u001b[38;5;28mint\u001b[39m(box[\u001b[38;5;241m2\u001b[39m]), \u001b[38;5;28mint\u001b[39m(box[\u001b[38;5;241m3\u001b[39m]))\n\u001b[0;32m--> 315\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mrectangle(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim, p1, p2, color, thickness\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlw, lineType\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mLINE_AA)\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label:\n\u001b[1;32m    317\u001b[0m     w, h \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mgetTextSize(label, \u001b[38;5;241m0\u001b[39m, fontScale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msf, thickness\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# text width, height\u001b[39;00m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'rectangle'\n> Overload resolution failed:\n>  - Scalar value for argument 'color' is not numeric\n>  - Scalar value for argument 'color' is not numeric\n>  - argument for rectangle() given by name ('thickness') and position (4)\n>  - argument for rectangle() given by name ('thickness') and position (4)\n"
     ]
    }
   ],
   "source": [
    "test_img = cv2.imread(test_file_list[10])\n",
    "img_src = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img_src)\n",
    "plt.show()\n",
    "results = model(img_src)[0]\n",
    "\n",
    "annotator = Annotator(img_src)\n",
    "boxes = results.boxes\n",
    "\n",
    "for box in boxes:\n",
    "    b = box.xyxy[0]\n",
    "    cls = box.cls\n",
    "    annotator.box_label(b, model.names[int(cls)], color_dict[int(cls)])\n",
    "\n",
    "img_src = annotator.result()\n",
    "plt.imshow(img_src)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
